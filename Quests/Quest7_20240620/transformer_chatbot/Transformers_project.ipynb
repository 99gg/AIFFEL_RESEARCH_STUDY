{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ea93127",
   "metadata": {},
   "source": [
    "영어로 만들었던 챗봇을 한국어 데이터로 바꿔서 훈련시켜봅시다.\n",
    "\n",
    "시작하기 전에 우선 주요 라이브러리 버전을 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd82c12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc3da97",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 수집하기\n",
    "\n",
    "한국어 챗봇 데이터는 송영숙님이 공개한 챗봇 데이터를 사용합니다.\n",
    "\n",
    "이 데이터는 아래의 링크에서 다운로드할 수 있습니다.\n",
    "\n",
    "    songys/Chatbot_data\n",
    "\n",
    "Cloud shell에서 아래 명령어를 입력해 주세요.\n",
    "```\n",
    "$ mkdir -p ~/aiffel/transformer_chatbot/data/\n",
    "$ ln -s ~/data/* ~/aiffel/transformer_chatbot/data/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ddac719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2570869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model) # (1, position, d_model), dtype=float32 \n",
    "    \n",
    "    # pos/10000^(2i/d_model) 를 반환\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, \n",
    "                            (2 * (i // 2)) / tf.cast(d_model, tf.float32)\n",
    "                           )\n",
    "        return position * angles # (position, d_model)\n",
    "        \n",
    "    def positional_encoding(self, position, d_model):\n",
    "        # 각도 배열 생성\n",
    "        angle_rads = self.get_angles(\n",
    "            # tf.newaxis : 기존 텐서에 새 차원을 추가\n",
    "            position = tf.range(position, dtype=tf.float32)[:, tf.newaxis], # (position, 1)\n",
    "            i = tf.range(d_model, dtype=tf.float32)[tf.newaxis, :], # (1, d_model)\n",
    "            d_model=d_model) \n",
    "        # angle_rads : (position, d_model)\n",
    "        \n",
    "        # 배열의 짝수 인덱스 : sin함수 적용\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2]) # 짝수번쨰(0, 2, 4, ....) : (position, d_model/2)\n",
    "        # 배열의 홀수 인덱스 : cos함수 적용\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2]) # 홀수번쨰(1, 3, 5, ....) : (position, d_model/2)\n",
    "        \n",
    "        # sine, cosine이 교차되도록 재배열\n",
    "        pos_encoding = tf.stack([sines, cosines], axis=0) # 각 연산이 된 결과들을 쌓는다. : (2, position, d_model/2)\n",
    "        pos_encoding = tf.transpose(pos_encoding, [1, 2, 0]) # 순서를 바꾼다. : (position, d_model/2, 2)\n",
    "        pos_encoding = tf.reshape(pos_encoding, [position, d_model]) # (position, d_model)\n",
    "        \n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...] # (1, position, d_model)\n",
    "        return tf.cast(pos_encoding, tf.float32) # type casting\n",
    "    \n",
    "    def call(self, inputs): # inputs을 (batch_size, seq_len, d_model) 이라고 하면(임베딩 후 결과)\n",
    "        \n",
    "        # [:, :tf.shape(inputs)[1], :] : slice (positional encoding) \n",
    "        # -> (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # inputs에 의거, broadcasting이 일어남(self.pos_encoding이 inputs 배치에 합연산)\n",
    "        \n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67815f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled Dot Product Attention\n",
    "def scaled_dot_product_attention(query, key, value, mask): # (seq_len, depth)\n",
    "    # Attention weight\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True) # (seq_len, seq_len)\n",
    "    \n",
    "    # Normalization\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32) # depth = d_model / num_heads\n",
    "    logits = matmul_qk / tf.math.sqrt(depth) # (seq_len, seq_len)\n",
    "    \n",
    "    # Add mask to Padding\n",
    "    if mask is not None: \n",
    "        logits += (mask * -1e9)\n",
    "        \n",
    "    # softmax\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1) # query에 대한 key의 softmax값 \n",
    "    \n",
    "    # Scaled Dot Product\n",
    "    output = tf.matmul(attention_weights, value) # (seq_len, seq_len) x (seq_len, depth)\n",
    "    return output # (seq_len, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10d9a4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "        \n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        # to use scaled_dot_product_attention\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3]) \n",
    "        \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        \n",
    "        # Q, K, V - Dense # (bs, seq_len, d_model)\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "         \n",
    "        # Parallel Computing - Multi heads : (batch_size, num_heads, seq_len, depth)\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "        \n",
    "        # scaled_attention\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "        \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) # (batch_size, seq_len, num_heads, depth)\n",
    "        \n",
    "        # concatenate\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                     (batch_size, -1, self.d_model)) # (batch_size, seq_len, d_model)\n",
    "        # Dense for final\n",
    "        outputs = self.dense(concat_attention)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b9012f",
   "metadata": {},
   "source": [
    "### 견해\n",
    "- 개인적으로 가장 어려웠던 것이 Multihead Attention 구현체였음.\n",
    "- 왜 ? : 노드에서 빈칸을 뚫어놨을 때 채우기 위한 생각도 많이 해야만 했고, 개별적으로 입력되는 input들이 각 라인 별 함수를 거쳐서 변하는 Tensor의 shape가 추적이 직관적으로 되지 않았기 때문, 또한 transformer 정의 후 model.summary()를 함에 있어서 모든 것이 조화롭지않으면 에러가 뜨게되는데, 해당 구현체의 세부함수에서 주로 에러가 발생했었음.\n",
    "\n",
    "나의 경우, Parallel computing을 클래스함수 호출(self.split_heads)을 해야하는데 그냥 split_heads를 해버려서 에러가 나기도 했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ba39cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "        \n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        # to use scaled_dot_product_attention\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3]) \n",
    "        \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        \n",
    "        # Q, K, V - Dense # (bs, seq_len, d_model)\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "         \n",
    "        # Parallel Computing - Multi heads : (batch_size, num_heads, seq_len, depth)\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "        \n",
    "        # scaled_attention\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "        \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) # (batch_size, seq_len, num_heads, depth)\n",
    "        \n",
    "        # concatenate\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                     (batch_size, -1, self.d_model)) # (batch_size, seq_len, d_model)\n",
    "        # Dense for final\n",
    "        outputs = self.dense(concat_attention)\n",
    "        \n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13f79fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32) # 0일 경우에 1(padding)로 표시\n",
    "#     print(mask.shape)\n",
    "    # (batch_size, 1, 1, seq_len)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84a216a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x): # (batch_size, seq_len, d_model)\n",
    "    seq_len = tf.shape(x)[1] \n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cd7040",
   "metadata": {},
   "source": [
    "- Padding mask : seq_len의 길이의 행렬이지만, 실제로는 값이 없는 곳을 나타내어 연산효율\n",
    "- Look ahead mask : 디코더의 연산을 순차적으로 진행하기 위해서 미래 시점의 정보를 가림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e8b27a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder layer implementation for a function\n",
    "\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    \n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "    \n",
    "    # First layer : Multi Head Self Attention \n",
    "    attention = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention\")({\n",
    "        'query': inputs,\n",
    "        'key': inputs,\n",
    "        'value': inputs,\n",
    "        'mask': padding_mask\n",
    "    })\n",
    "    \n",
    "    # Dropout, Layer Normalization(Residual connection)\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(inputs + attention) \n",
    "    # epsilon -  Small float added to variance to avoid dividing by zero. Defaults to 1e-3. \n",
    "    \n",
    "    # Second layer : 2 Fully-connected Layer\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "        \n",
    "    # Dropout, Layer Normalization(Residual connection)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention + outputs) \n",
    "    \n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62b82d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(vocab_size,\n",
    "           num_layers,\n",
    "           units,\n",
    "           d_model,\n",
    "           num_heads,\n",
    "           dropout,\n",
    "           name=\"encoder\"):\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    \n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "    \n",
    "    # Embedding layer\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    # Scale Normalizing for positional encoding\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32)) \n",
    "    \n",
    "    # Positional encoding\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    \n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "    \n",
    "    # Encoder layer for num_layers\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(\n",
    "                units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"encoder_layer_{}\".format(i),\n",
    "        )([outputs, padding_mask])\n",
    "        \n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name\n",
    "        \n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d643bd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder layer implementation for a function\n",
    "\n",
    "# 3 sublayers included\n",
    "\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    # encoder의 self attention\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name=\"look_ahead_mask\"\n",
    "    )\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "    \n",
    "    # Fisrt sub_layer : Masked Multihead Self Attention\n",
    "    attention1 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "        'query': inputs,\n",
    "        'key': inputs,\n",
    "        'value': inputs,\n",
    "        'mask': look_ahead_mask        \n",
    "        })\n",
    "    \n",
    "    # LayerNormalization\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention1 + inputs)\n",
    "    \n",
    "    # Second sub_layer : Encoder-decoder Attention\n",
    "    attention2 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "        'query': attention1,\n",
    "        'key': enc_outputs,\n",
    "        'value': enc_outputs,\n",
    "        'mask': padding_mask\n",
    "        })\n",
    "    \n",
    "    # Dropout, Layer Normalization(Residual connection)\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention2 + attention1)\n",
    "    \n",
    "    # Third layer : 2 Fully-connected Layer\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "        \n",
    "    # Dropout, Layer Normalization(Residual connection)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(outputs + attention2) \n",
    "    \n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask], \n",
    "        outputs=outputs, \n",
    "        name=name\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fb8bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "    # 패딩 마스크\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name='decoder_layer_{}'.format(i),\n",
    "            )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01ca86ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "               num_layers,\n",
    "               units,\n",
    "               d_model,\n",
    "               num_heads,\n",
    "               dropout,\n",
    "               name=\"transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "    \n",
    "    enc_padding_mask = tf.keras.layers.Lambda( # 함수 기반으로 레이어 생성\n",
    "        create_padding_mask, # (1, 1, seq_len)의 반환값\n",
    "        output_shape = (1, 1, None),\n",
    "        name='enc_padding_mask')(inputs)\n",
    "    \n",
    "    # for decoder masking \n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask,\n",
    "        output_shape=(1, None, None),\n",
    "        name='look_ahead_mask')(dec_inputs)\n",
    "    \n",
    "    # for Decoder padding\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, \n",
    "        output_shape=(1, 1, None),\n",
    "        name='dec_padding_mask')(inputs)\n",
    "    \n",
    "    # Encoder\n",
    "    enc_outputs = encoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[inputs, enc_padding_mask])\n",
    "    \n",
    "    # Decoder\n",
    "    dec_outputs = decoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "    \n",
    "    # Fully connected layer\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "    \n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bc141d",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 전처리하기\n",
    "\n",
    "영어 데이터와는 전혀 다른 데이터인 만큼 영어 데이터에 사용했던 전처리와 일부 동일한 전처리도 필요하겠지만 전체적으로는 다른 전처리를 수행해야 할 수도 있습니다.\n",
    "\n",
    "# <span style=\"color:red\"> -----여기서부터는 진행을 못했습니다. ------</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ef4a06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'cornell_movie_dialogs.zip',\n",
    "    origin='http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_dataset = os.path.join(\n",
    "    os.path.dirname(path_to_zip), \"cornell movie-dialogs corpus\")\n",
    "\n",
    "path_to_movie_lines = os.path.join(path_to_dataset, 'movie_lines.txt')\n",
    "path_to_movie_conversations = os.path.join(path_to_dataset,'movie_conversations.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "050727bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "# 사용할 샘플의 최대 개수\n",
    "MAX_SAMPLES = 50000\n",
    "print(MAX_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9829b408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리(클리닝) 함수\n",
    "def preprocess_sentence(sentence):\n",
    "    # 입력받은 sentence를 소문자로 변경하고 양쪽 공백을 제거\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.strip() # 양쪽 공백을 제거\n",
    "\n",
    "    # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
    "    # 예를 들어서 \"I am a student.\" => \"I am a student .\"와 같이\n",
    "    # student와 온점 사이에 거리를 만듭니다.\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "    # (a-z, A-Z, \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백인 ' '로 대체합니다. \n",
    "    sentence = re.sub(\"[^a-zA-Z.?!,]\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c847590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문과 답변의 쌍인 데이터셋을 구성하기 위한 데이터 로드 함수\n",
    "def load_conversations():\n",
    "    id2line = {}\n",
    "    with open(path_to_movie_lines, errors='ignore') as file:\n",
    "        lines = file.readlines()\n",
    "    for line in lines:\n",
    "        parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "        id2line[parts[0]] = parts[4]\n",
    "\n",
    "    inputs, outputs = [], []\n",
    "    with open(path_to_movie_conversations, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "        conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]\n",
    "    \n",
    "        for i in range(len(conversation) - 1):\n",
    "            # 전처리 함수를 질문에 해당되는 inputs와 답변에 해당되는 outputs에 적용.\n",
    "            inputs.append(preprocess_sentence(id2line[conversation[i]]))\n",
    "            outputs.append(preprocess_sentence(id2line[conversation[i + 1]]))\n",
    "\n",
    "        if len(inputs) >= MAX_SAMPLES:\n",
    "            return inputs, outputs\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88a0315a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 50000\n",
      "전체 샘플 수 : 50000\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 로드하고 전처리하여 질문을 questions, 답변을 answers에 저장합니다.\n",
    "questions, answers = load_conversations()\n",
    "print('전체 샘플 수 :', len(questions))\n",
    "print('전체 샘플 수 :', len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6e5e533",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후의 22번째 질문 샘플: she s not a . . .\n",
      "전처리 후의 22번째 답변 샘플: lesbian ? no . i found a picture of jared leto in one of her drawers , so i m pretty sure she s not harboring same sex tendencies .\n"
     ]
    }
   ],
   "source": [
    "print('전처리 후의 22번째 질문 샘플: {}'.format(questions[21]))\n",
    "print('전처리 후의 22번째 답변 샘플: {}'.format(answers[21]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b119bc7",
   "metadata": {},
   "source": [
    "# 데이터 전처리\n",
    "\n",
    "\n",
    "- TensorFlow Datasets SubwordTextEncoder 를 토크나이저로 사용한다.  단어보다 더 작은 단위인 Subword를 기준으로 토크나이징하고,  각 토큰을 고유한 정수로 인코딩 한다.\n",
    "- 각 문장을 토큰화하고 각 문장의 시작과 끝을 나타내는 START_TOKEN 및 END_TOKEN을 추가한다.\n",
    "- 최대 길이 MAX_LENGTH 인 40을 넘는 문장들은 필터링한다.\n",
    "- MAX_LENGTH보다 길이가 짧은 문장들은 40에 맞도록 패딩 한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "811d764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76d76a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [8336]\n",
      "END_TOKEN의 번호 : [8337]\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "# 시작, 토큰에 부여된 정수를 출력\n",
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c9e48cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8338\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9076b2",
   "metadata": {},
   "source": [
    "위에서 tensorflow_datasets의 SubwordTextEncoder를 사용해서 tokenizer를 정의하고 Vocabulary를 만들었다면, \n",
    "tokenizer.encode()로 각 단어를 정수로 변환할 수 있고 또는 tokenizer.decode()를 통해 정수 시퀀스를 단어 시퀀스로 변환할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bdb7ce88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 21번째 질문 샘플: [62, 8, 36, 8177, 50]\n",
      "정수 인코딩 후의 21번째 답변 샘플: [7835, 1126, 19, 59, 2, 4, 340, 10, 1683, 14, 1114, 749, 2607, 272, 17, 74, 14, 109, 2162, 864, 3, 63, 4, 23, 357, 208, 62, 8, 36, 901, 2310, 8112, 347, 1040, 5187, 4232, 335, 1]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
    "# 각 토큰을 고유한 정수로 변환\n",
    "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(questions[21])))\n",
    "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(answers[21])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f010244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이\n",
    "MAX_LENGTH = 40\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "efd3023d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    # 최대 길이 40 이하인 경우에만 데이터셋으로 허용\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "      tokenized_inputs.append(sentence1)\n",
    "      tokenized_outputs.append(sentence2)\n",
    "  \n",
    "  # 최대 길이 40으로 모든 데이터셋을 패딩\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "  return tokenized_inputs, tokenized_outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf7a9489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 8338\n",
      "필터링 후의 질문 샘플 개수: 43982\n",
      "필터링 후의 답변 샘플 개수: 43982\n"
     ]
    }
   ],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0f18c1",
   "metadata": {},
   "source": [
    "# 교사 강요(Teacher Forcing)\n",
    "tf.data.Dataset API 는 훈련 프로세스의 속도가 빨라지도록 입력 파이프라인을 구축하는 API입니다.\n",
    "\n",
    "이를 적극 사용하기 위해서 질문과 답변의 쌍을 tf.data.Dataset의 입력으로 넣어주는 작업을 합니다.\n",
    "\n",
    "이때, 디코더의 입력과 실제값(레이블)을 정의해 주기 위해서는 교사 강요(Teacher Forcing) 이라는 언어 모델의 훈련 기법을 이해해야만 합니다.\n",
    "\n",
    "- 교사 강요를 사용하면, t+1에서 예측 시 t까지의 정보를 모델의 예측값을 이용하는 것이 아닌 실제 레이블을 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c7ee664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M3 Pro\n",
      "\n",
      "systemMemory: 36.00 GB\n",
      "maxCacheSize: 13.50 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-21 09:35:52.610770: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-06-21 09:35:52.611311: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
    "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE) # 학습 시 다음 데이터 배치를 준비하도록 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c78a78",
   "metadata": {},
   "source": [
    "## Step 3. SubwordTextEncoder 사용하기\n",
    "\n",
    "한국어 데이터는 형태소 분석기를 사용하여 토크나이징을 해야 한다고 많은 분이 알고 있습니다. 하지만 여기서는 형태소 분석기가 아닌 위 실습에서 사용했던 내부 단어 토크나이저인 SubwordTextEncoder를 그대로 사용해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9154aa4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2f34464",
   "metadata": {},
   "source": [
    "## Step 4. 모델 구성하기\n",
    "\n",
    "위 실습 내용을 참고하여 트랜스포머 모델을 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50375a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 세션에 존재하는 모든 텐서 객체들을 삭제 - 메모리 누수 방지 및 이전에 사용된 가중치 초기화\n",
    "tf.keras.backend.clear_session() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4cb20aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파리마터\n",
    "NUM_LAYERS = 2 # 인코더, 디코더 층의 개수\n",
    "D_MODEL = 256 # 인코더, 디코더 내부 입,출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서 헤드 수\n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ff254b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " inputs (InputLayer)            [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " dec_inputs (InputLayer)        [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " enc_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n",
      "                                                                                                  \n",
      " encoder (Functional)           (None, None, 256)    3188736     ['inputs[0][0]',                 \n",
      "                                                                  'enc_padding_mask[0][0]']       \n",
      "                                                                                                  \n",
      " look_ahead_mask (Lambda)       (None, 1, None, Non  0           ['dec_inputs[0][0]']             \n",
      "                                e)                                                                \n",
      "                                                                                                  \n",
      " dec_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n",
      "                                                                                                  \n",
      " decoder (Functional)           (None, None, 256)    3716096     ['dec_inputs[0][0]',             \n",
      "                                                                  'encoder[0][0]',                \n",
      "                                                                  'look_ahead_mask[0][0]',        \n",
      "                                                                  'dec_padding_mask[0][0]']       \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, None, 8338)   2142866     ['decoder[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 9,047,698\n",
      "Trainable params: 9,047,698\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE, \n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18eed32",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1d4062c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, \n",
    "                                       MAX_LENGTH - 1)) # Time step ?\n",
    "    \n",
    "    if tf.__version__ >= '2.9.0' : \n",
    "            \n",
    "        # print(type(mask), type(loss)) \n",
    "        # <class 'tensorflow.python.framework.ops.Tensor'> <class 'keras.losses.SparseCategoricalCrossentropy'>\n",
    "        # 로컬 MacbookPro tf v2.9 기준 type의 불일치로 연산이 진행되지 않음\n",
    "        \n",
    "        loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, \n",
    "            reduction='none',\n",
    "            ) # 각 샘플에 대한 손실값을 반환\n",
    "        \n",
    "        loss = loss_obj(y_true, y_pred)\n",
    "        loss = tf.cast(loss, dtype=tf.float32)\n",
    "        \n",
    "    \n",
    "    else : \n",
    "        \n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, \n",
    "            reduction='none') # 각 샘플에 대한 손실값을 반환\n",
    "        (y_true, y_pred)\n",
    "        \n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32) # 패딩 토큰을 고려    \n",
    "    loss = tf.multiply(loss, mask)\n",
    "    return tf.reduce_mean(loss) # 실제 고려된 손실\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baab433",
   "metadata": {},
   "source": [
    "# Customized Learning rate\n",
    "$$\\text{learning_rate}=d^{−0.5}_{model}⋅min(\\text{step_num}^{−0.5},\\text{step_num⋅warmup_steps}^{−1.5}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cc4c5165",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f628380a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c662c0b0",
   "metadata": {},
   "source": [
    "# Model Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "731f2707",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "32c6ee55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 48/688 [=>............................] - ETA: 58s - loss: 0.9829 - accuracy: 0.1221"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/aiffel/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/aiffel/lib/python3.9/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/aiffel/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/aiffel/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/aiffel/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/aiffel/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/aiffel/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/aiffel/lib/python3.9/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/aiffel/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca3cd26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0817840d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15ee958a",
   "metadata": {},
   "source": [
    "## Step 5. 모델 평가하기\n",
    "\n",
    "Step 1에서 선택한 전처리 방법을 고려하여 입력된 문장에 대해서 대답을 얻는 예측 함수를 만듭니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a83778f",
   "metadata": {},
   "source": [
    "# 챗봇 테스트하기\n",
    "예측(inference) 단계는 기본적으로 다음과 같은 과정을 거칩니다.\n",
    "\n",
    "    새로운 입력 문장에 대해서는 훈련 때와 동일한 전처리를 거친다.\n",
    "    입력 문장을 토크나이징하고, START_TOKEN과 END_TOKEN을 추가한다.\n",
    "    패딩 마스킹과 룩 어헤드 마스킹을 계산한다.\n",
    "    디코더는 입력 시퀀스로부터 다음 단어를 예측한다.\n",
    "    디코더는 예측된 다음 단어를 기존의 입력 시퀀스에 추가하여 새로운 입력으로 사용한다.\n",
    "    END_TOKEN이 예측되거나 문장의 최대 길이에 도달하면 디코더는 동작을 멈춘다.\n",
    "\n",
    "위의 과정을 모두 담은 decoder_inference() 함수를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e265ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "    # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "    sentence = tf.expand_dims(\n",
    "        START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "    # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "    # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "    output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "    # 디코더의 인퍼런스 단계\n",
    "    for i in range(MAX_LENGTH):\n",
    "        # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "        predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "\n",
    "        # 현재 예측한 단어의 정수\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "        # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "        # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output_sequence, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a10047a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(sentence):\n",
    "    # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "    prediction = decoder_inference(sentence)\n",
    "\n",
    "    # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "    print('입력 : {}'.format(sentence))\n",
    "    print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e52244e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : Where have you been?\n",
      "출력 : i m not on the phone .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i m not on the phone .'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('Where have you been?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "38231f2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : It's a trap\n",
      "출력 : i don t want to be a good man .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i don t want to be a good man .'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"It's a trap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4582eb1",
   "metadata": {},
   "source": [
    "# 회고\n",
    "- 2024년 6월 20일 16시 55분 기준, 예제 코드는 다 끝냈지만, 프로젝트 데이터는 이제 시작을 해야하는 상황임.\n",
    "- 다행인 건 Transformer 모델에 대해서는 잘 이해할 수 있었다.\n",
    "- 하지만, 개별 구성요소들이 최소 함수 혹은 클래스 구현체로 되어있고, 이것들의 연계가 아직까지는 빠르게 읽히지 않음.\n",
    "- 또한, (batch_size, sequence_length, dimension_of_model) 단위(최소 2차원) 텐서 연산이 머릿속에 잘 안그려진다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7315f1a6",
   "metadata": {},
   "source": [
    "- MacOS 로컬(본인 노트북)에서 tensorflow v2.9를 활용해서 모델 학습에 성공하였다. \n",
    "    - v2.6을 처음에 시도하여 최대한 버전을 LMS와 유사하게 만들어보려고 했지만, 하위 패키지 dependencies 문제가 \n",
    "    발생했었기 때문에 v2.16이 아닌 멀지 않은 상위버전(v2.9) 을 선택하였음.\n",
    "\n",
    "- loss함수에서 error를 확인하였으며, 버전 변화에 따라 텐서 타입에 대한 변형이 좀 강제화되었던 느낌이었고, \n",
    "처음 겪어본 에러라 처리하는데에 시간이 좀 많이 소요되었다. 인스턴스 선언과 손실값 계산, type casting을 각각 분리해서 진행해서 해결하였고, 결국 해결이 되어서 기분이 좋았음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6c3962",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
